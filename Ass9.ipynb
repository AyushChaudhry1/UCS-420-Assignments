{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272e9dc3-e1ef-48c4-905d-135d4d7c8592",
   "metadata": {},
   "source": [
    "# Assignment 9 \n",
    "<b>Question 1</b> Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).<br><br>\n",
    "Convert text to lowercase and remove punctuation.<br>\n",
    "Tokenize the text into words and sentences.<br>\n",
    "Remove stopwords (using NLTK's stopwords list).<br>\n",
    "Display word frequency distribution (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95eae843-53ed-4557-b2a6-50ebc4bac9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Adminn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Adminn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Adminn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Adminn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus   import stopwords\n",
    "from nltk.stem     import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c9238a-c537-43af-864a-a161e62acb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food is essential for our survival and health.  \n",
      "It provides energy, nutrients, and keeps our bodies strong.  \n",
      "Different cultures have unique and delicious cuisines.  \n",
      "Fresh fruits, vegetables, and grains are very nutritious.  \n",
      "Food also brings people together during meals and celebrations.  \n",
      "Eating a balanced diet helps us live a happy, healthy life.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "new_paragraph = \"\"\"Food is essential for our survival and health.  \n",
    "It provides energy, nutrients, and keeps our bodies strong.  \n",
    "Different cultures have unique and delicious cuisines.  \n",
    "Fresh fruits, vegetables, and grains are very nutritious.  \n",
    "Food also brings people together during meals and celebrations.  \n",
    "Eating a balanced diet helps us live a happy, healthy life.\"\"\"\n",
    "new_lines = new_paragraph.split(\"\\n\")\n",
    "print(new_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f843d603-f384-414f-8f11-c27d83c9a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['food', 'essential', 'survival', 'health', 'provides', 'energy', 'nutrients', 'keeps', 'bodies', 'strong', 'different', 'cultures', 'unique', 'delicious', 'cuisines', 'fresh', 'fruits', 'vegetables', 'grains', 'nutritious', 'food', 'also', 'brings', 'people', 'together', 'meals', 'celebrations', 'eating', 'balanced', 'diet', 'helps', 'us', 'live', 'happy', 'healthy', 'life']\n",
      "Word Frequencies:\n",
      "FreqDist({'food': 2, 'essential': 1, 'survival': 1, 'health': 1, 'provides': 1, 'energy': 1, 'nutrients': 1, 'keeps': 1, 'bodies': 1, 'strong': 1, ...})\n"
     ]
    }
   ],
   "source": [
    "processed_text = new_paragraph.lower()\n",
    "processed_text = processed_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "individual_tokens = word_tokenize(processed_text)\n",
    "sentence_units = sent_tokenize(processed_text)\n",
    "\n",
    "common_words = set(stopwords.words(\"english\"))\n",
    "meaningful_words = [item for item in individual_tokens if item not in common_words]\n",
    "\n",
    "frequency_distribution = FreqDist(meaningful_words)\n",
    "print(\"Filtered Words:\", meaningful_words)\n",
    "print(\"Word Frequencies:\")\n",
    "frequency_distribution.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678e031-e446-438e-9d24-5524ee740e3c",
   "metadata": {},
   "source": [
    "<b> Question 2</b> Stemming and Lemmatization:<br>\n",
    "Take the tokenized words from Question 1 (after stopword removal).<br>\n",
    "Apply stemming using NLTK's PorterStemmer and LancasterStemmer.<br>\n",
    "Apply lemmatization using NLTK's WordNetLemmatizer.<br>\n",
    "Compare and display results of both techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00bb3ab3-89ec-4cb2-b25b-c5e855b47bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['food', 'essenti', 'surviv', 'health', 'provid', 'energi', 'nutrient', 'keep', 'bodi', 'strong', 'differ', 'cultur', 'uniqu', 'delici', 'cuisin', 'fresh', 'fruit', 'veget', 'grain', 'nutriti', 'food', 'also', 'bring', 'peopl', 'togeth', 'meal', 'celebr', 'eat', 'balanc', 'diet', 'help', 'us', 'live', 'happi', 'healthi', 'life']\n",
      "\n",
      "Lancaster Stemmer: ['food', 'ess', 'surv', 'heal', 'provid', 'energy', 'nutry', 'keep', 'body', 'strong', 'diff', 'cult', 'un', 'delicy', 'cuisin', 'fresh', 'fruit', 'veget', 'grain', 'nut', 'food', 'also', 'bring', 'peopl', 'togeth', 'meal', 'celebr', 'eat', 'bal', 'diet', 'help', 'us', 'liv', 'happy', 'healthy', 'lif']\n",
      "\n",
      "Lemmatized: ['food', 'essential', 'survival', 'health', 'provides', 'energy', 'nutrient', 'keep', 'body', 'strong', 'different', 'culture', 'unique', 'delicious', 'cuisine', 'fresh', 'fruit', 'vegetable', 'grain', 'nutritious', 'food', 'also', 'brings', 'people', 'together', 'meal', 'celebration', 'eating', 'balanced', 'diet', 'help', 'u', 'live', 'happy', 'healthy', 'life']\n"
     ]
    }
   ],
   "source": [
    "port_stemmer = PorterStemmer()\n",
    "lanc_stemmer = LancasterStemmer()\n",
    "word_net_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "port_result = [port_stemmer.stem(item) for item in meaningful_words]\n",
    "lanc_result = [lanc_stemmer.stem(item) for item in meaningful_words]\n",
    "lemma_output = [word_net_lemmatizer.lemmatize(item) for item in meaningful_words]\n",
    "\n",
    "# PorterStemmer: A rule-based stemming algorithm that removes suffixes to reduce words to their root form. It follows a set of predefined rules and does not require training.\n",
    "# LancasterStemmer: A more aggressive rule-based stemming algorithm that performs faster but may be harsher in its reductions compared to the Porter Stemmer.\n",
    "# WordNetLemmatizer: Uses the WordNet lexical database to reduce words to their lemma form (dictionary form). It requires knowledge of the word's part of speech (POS) for accuracy.\n",
    "\n",
    "print(\"Porter Stemmer:\", port_result)\n",
    "print(\"\\nLancaster Stemmer:\", lanc_result)\n",
    "print(\"\\nLemmatized:\", lemma_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7191bb41-e71c-4f0d-a74e-cdd12d67f798",
   "metadata": {},
   "source": [
    "<b>Question 3</b> Regular Expressions and Text Splitting:<br>\n",
    "Take the original text from Question 1.<br>\n",
    "Use regular expressions to:<br>\n",
    "Extract all words with more than 5 letters.<br>\n",
    "Extract all numbers (if any exist in their text).<br>\n",
    "Extract all capitalized words.<br>\n",
    "Use text splitting techniques to:<br>\n",
    "Split the text into words containing only alphabets (removing digits and special characters).<br>\n",
    "Extract words starting with a vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4171b7c9-0c2a-418d-9428-ef6aee804b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words > 5 letter length: ['essential', 'survival', 'health', 'provides', 'energy', 'nutrients', 'bodies', 'strong', 'Different', 'cultures', 'unique', 'delicious', 'cuisines', 'fruits', 'vegetables', 'grains', 'nutritious', 'brings', 'people', 'together', 'during', 'celebrations', 'Eating', 'balanced', 'healthy']\n",
      "\n",
      "Numbers: []\n",
      "\n",
      "Capitalized words: ['Food', 'It', 'Different', 'Fresh', 'Food', 'Eating']\n",
      "\n",
      "Alphabet-only words: ['Food', 'is', 'essential', 'for', 'our', 'survival', 'and', 'health', 'It', 'provides', 'energy', 'nutrients', 'and', 'keeps', 'our', 'bodies', 'strong', 'Different', 'cultures', 'have', 'unique', 'and', 'delicious', 'cuisines', 'Fresh', 'fruits', 'vegetables', 'and', 'grains', 'are', 'very', 'nutritious', 'Food', 'also', 'brings', 'people', 'together', 'during', 'meals', 'and', 'celebrations', 'Eating', 'a', 'balanced', 'diet', 'helps', 'us', 'live', 'a', 'happy', 'healthy', 'life']\n",
      "\n",
      "Words starting with vowels: ['is', 'essential', 'our', 'and', 'It', 'energy', 'and', 'our', 'unique', 'and', 'and', 'are', 'also', 'and', 'Eating', 'a', 'us', 'a']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "input_string = new_paragraph\n",
    "\n",
    "long_strings = re.findall(r'\\b\\w{6,}\\b', input_string)\n",
    "\n",
    "found_numbers = re.findall(r'\\d+', input_string)\n",
    "\n",
    "leading_capital_words = re.findall(r'\\b[A-Z][a-z]*\\b', input_string)\n",
    "\n",
    "pure_alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', input_string)\n",
    "\n",
    "starting_vowel_words = [item for item in pure_alpha_words if item[0].lower() in 'aeiou']\n",
    "\n",
    "print(\"Words > 5 letter length:\", long_strings)\n",
    "print(\"\\nNumbers:\", found_numbers)\n",
    "print(\"\\nCapitalized words:\", leading_capital_words)\n",
    "print(\"\\nAlphabet-only words:\", pure_alpha_words)\n",
    "print(\"\\nWords starting with vowels:\", starting_vowel_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff41e7f-ee23-4b66-82cf-7a15a9e2202b",
   "metadata": {},
   "source": [
    "<b>Question 4</b> Custom Tokenization & Regex-based Text Cleaning:<br>\n",
    "Take original text from Question 1<br>\n",
    "Write a custom tokenization function that:<br>\n",
    "Removes punctuation and special symbols, but keeps contractions (e.g., \"isn't\" should not be split into \"is\" and \"n't\").<br>\n",
    "Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).<br>\n",
    "Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as it is).<br>\n",
    "Use Regex Substitutions (re.sub) to:<br>\n",
    "Replace email addresses with <EMAIL> placeholder.<br>\n",
    "Replace URLs with <URL> placeholder.<br>\n",
    "Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with <PHONE> placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "116d9f23-1b62-4fe6-9cdd-c1f6909c08c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tokens: ['Contact', 'us', 'at', 'support', 'website', 'com', 'See', 'more', 'at', 'https', 'website', 'info', 'or', 'ring', 'us', 'at', '91', '7777777777']\n",
      "Cleaned Text: Contact us at <EMAIL> See more at <URL> or ring us at <PHONE>.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "example_string = \"Contact us at support@website.com. See more at https://website.info or ring us at +91 7777777777.\"\n",
    "\n",
    "def specialized_token_extraction(text):\n",
    "    return re.findall(r\"\\b\\w+(?:[-']\\w+)*\\b|\\d+\\.\\d+|\\d+\", text)\n",
    "\n",
    "generated_tokens = specialized_token_extraction(example_string)\n",
    "\n",
    "modified_text = re.sub(r'\\S+@\\S+', '<EMAIL>', example_string)\n",
    "modified_text = re.sub(r'https?://\\S+', '<URL>', modified_text)\n",
    "modified_text = re.sub(r'\\+91\\s?\\d{10}|\\d{3}-\\d{3}-\\d{4}', '<PHONE>', modified_text)\n",
    "\n",
    "print(\"Custom Tokens:\", generated_tokens)\n",
    "print(\"Cleaned Text:\", modified_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e3507-d236-4d13-8441-d8a43e661b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
